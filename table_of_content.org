Diplomarbeit Stichpunkte
Erik Zenker

Table of contents
=================
* Things to think about
  - Chapter about efficiency / requirements of communication
  

* Introduction
  - Simulations need more computational power
  - Super computers faster, more powerful
  - More hardware --> more failures
  - More hardware --> bigger simulation
  - PIConGPU runs on super computer with 18K nodes
  - Bigger simulation --> better load distribution
  - Communication model is foundation for *fault tolerance* and *load balance*
  - MPI is very stodgy and inflexible
  - Need to abstract from MPI as communication middleware
    --> Makes it possible to exchange it
    --> Use of communication layer which has more
        wanted propertiers
  - Provide a general communication layer --> *commuication based on graphs*
  - Main emphasis on dynamic communication framework
    --> ready for fault tolerance and load balancing
  - Communication on distributed Memmory systems (why?)


* Multiple-processor computer systems
  - Introduction on parallel computation and its varying models
  - Description of communication models 

** Tightly coupled
   Multiple CPUs connected at bus level
   --> shared memory system
   --> parallel computing

** Loosely coupled (Computer cluster)
   - Multiple CPUs connected via a high speed communication system
   - Computation-intensive purposes
   - A lot of 500 fastest supercomputers are cluster systems
   --> Distributed memory system
   --> Distributed computing (http://en.wikipedia.org/wiki/Distributed_processing)
       - Autonomous computational entities(nodes), each of which has its own local memory
       - Nodes communicates with each other by message passing
       - Tolerates failures in individual nodes
       - Nodes have incomplete view of the system
   --> Distributed systems
   --> Distributed shared memory (http://en.wikipedia.org/wiki/Distributed_shared_memory)
       - Physically separate memories can be addressed as one (logically shared) adress space
       - Provides by a library or the operating system
       - Examples : Kerrighed, OpenSSI, MOSIX, TreadMarks, DIPC
       - Grid Computing / Meta Computing as connection of clusters (supercomputers)
       --> Dynamically addition and deletion of calculation nodes
       --> Future of calculation ressources


* Traditional Communication Mechanisms for Clusters 
  - Its not about low level communication
  - Communication on top of existing communication mechanisms (overlay network, communication middleware)
  - Lightweight messaging Systems
** TCP, UDP, IP and Sockets
   - Unix like communication
   - Very low level
** RPC
   - Community of distributed systems
   - Communication in programming like flavor
** Message passing (http://en.wikipedia.org/wiki/Message_passing)
    - High level of abstraction
    - Fits needs of parallel programming community
    - Sending messages between processes on same or different nodes
    - high performance at a relatively low cost
**** MPI
     - Standardized and portable message-passing specification
     - De facto standard for communication
     - Implementations are available on virtually every parallel 
       computer systems
     - Overlay network implemented on top of the communication
       abstractraction provided by the system (sockets, AM, etc.)
     - Zero-Copy data transfer
     - Usable in shared and distributed memory systems
     - Free libraries available (OpenMPI)
     - Language independent communication protocol
     - Point-to-point communication
     - Collective communication
     - Barrier operation

     - Not very flexible --> static ranks
     - Makes runtime load balancing hard
     - No built-in fault tolerance
     - Does not offer any run-time management system
***** MPI-1 (MPI-1.3)
      - message passing
      - static runtime environment
***** MPI-2 (MPI-2.2)
      - parallel I/O
      - dynamic process management
      - remote memory operations
***** MPI-3 (MPI-3.0)
      - nonblocking collective operations
      - extensions to one-sided operations
***** Sources
      - http://en.wikipedia.org/wiki/Message_Passing_Interface
      - http://www.mcs.anl.gov/research/projects/mpi/
      - http://www.mpi-forum.org/
      -  High Performance Cluster Computing - Volume 1 : Architectures and Systems
      	 
** Parallel Virtual Machine
   - Parallel Virtual Machine connects a collection of heterogeneous computers
     to a single "parallel virtual machine"
     (http://en.wikipedia.org/wiki/PVM)
   - Support for communication and synchronization operations
   - Configuration control
   - synamic Spawning of pocressen
   - PVM daemons are spawned on a set of nodes
   - Local processes connect to PVM daemons and can 
     communicate through this daemon to other PVM daemons
*** Source
    - High performance cluster computing 1 : p. 522 - 21.3.2

** Active messages
   - no sender receiver like model
   - one-side communication paradigm
   - no reseive operation needed

** Shared memory

** Overlay network
*** Tree based overlay network
*** Distributed hash table (DHT)


* Fault tolerance
** Methods
*** Replication (http://en.wikipedia.org/wiki/Redundancy_(engineering))
    - duplication of critical components
*** Resilience
    - provide and maintain an acceptable level of service in face of faults
** Linux-HA (http://www.linux-ha.org/wiki/Main_Page)


* Load balancing
  - Dynamic load balancing needed 
  - Distribution of workload to processes of the network
  - Workload migration to other process
  - Application specific balancing system on top of an existing network-specific balancing system
    --> algorithms with mesh refinement lead to unbalanced load at runtime
    --> static load balancing not possible
  - balancing device(s) collects status information from compute nodes
    --> can be a single device or a group of devices (also in tree structure possible)
  - Balancing factors
    + 
  - Balancing methods
    + Weighting
    + Randomization
    + Round-Robin
    + Hashing
    + Fastest response
    + Nearest neighbor

** Source 
   - High Performance Cluster Computing - Volume 1 : Architectures and Systems, Kapitel 14


* Graph based communication
  - Overlay network on communication layer (MPI)
  - Based on mathematical model of graphs
  - Communicaton between adjacent nodes of a graph
  - Collective communication between all nodes of a graph
  - Creation of subgraphs
  - PVM uses some similar kind of model
    --> migration of processes to other PVM deamons
  - Foundation for fault tolerance and load balancing
  - Load sharing by hosting several vertices by the same host

** Graph
   - A graph is a set of vertices where some pairs of
     vertices are conntected by edges.
** Generic communicator
*** Communicator
    - Every process that wants to take part on communication in general,
      need an instance of the Communicator and is identifiable by a
      CommID (Communicator ID).
    - The Communicator provides standard p2p and collective operations
      on a high abstraction level (similar to boost::mpi interface)
    - The communication itself is implemented by the Communication
      policy.
**** Communication policy
     - Core communication class, implements communcation functionality
     - Implemented with MPI C bindings in the prototype
     - Also boost::mpi, boost::asio, ZMQ or P2P-overlay network imaginable
**** Context
     - A set of Communicators which are able to communicate with each other.
     - All Communicators of a context can perform a collective operation
**** P2P
     - Direct communication functions (send, recv) between Communicators
**** Collective
     - Collective communication functions
       + gather
       + gather2
       + allGather
       + allGather2
       + scatter
       + allToAll
       + reduce
       + allReduce
       + broadcast
       + synchronize
       + createContext
       + getGlobalContext
**** Event
     - Returned by non-blocking functions or asynchronous function
     - Events can be checked weather the the function has finished or not

*** Graph
    - Desciption of directed graphs
    - Vertices and edges are defined by properties
    - A Property is a struct / class that at least provide an id
    - Creation of subgraph
    - Deletion of vertices at runtime *TODO*
    - Adding vertices and edges at runtime *TODO*
**** GraphPolicy
     - Implements graph functionality
     - BGL

*** NameService
    - Connection between Graph and Communicator
    - Every Communicator announces its hosted vertices of a graph
    - Locate the host Communicator of vertices

*** GraphCommunicator
    - Provides point to point and collective communication schemas on graph base
    - Communicator is used as communication backend and NameService provide location information of the vertices of the Graph
    - Point to point communication between vertices
    - Collective operations on graphs

*** Game of Life
    - Simple example for generic communicator
    - Game field is modeled as 2D mesh graph
      0--1--2
      |\/|\/|
      |/\|/\|
      3--4--5
      |\/|\/|
      |/\|/\|
      6--7--8
    - Rules where take from : http://en.wikipedia.org/wiki/Conway's_Game_of_Life
    - Every Vertex calculates one Cell (Vertex == Cell)
    - Every cell is connected with its neighboring cells
    - One process calculates several cells

*** Redistribution of vertices
    - Vertices are not statically bounded to Communicators
    - Redistribution of vertices to a different host Communicator is possible
    - Redistribution within Communicators of a graph is no problem
    - Redistribution to Communicators outside of a graph needs
      recreation of the graph and reannounce *TODO*

*** Vertex resilience
    - The same vertex could be hosted by several Communicators
    - Sending data to a vertex will be transformed to a
      multicast operation
    - Builtin fault tolerance through copies of vertices


* Evaluation

* Benchmark


* Conclusion
