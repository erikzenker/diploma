\chapter{Introduction}
\label{sec:intro}

% Die Einleitung schreibt man zuletzt, wenn die Arbeit im Großen und
% Ganzen schon fertig ist. (Wenn man mit der Einleitung beginnt - ein
% häufiger Fehler - braucht man viel länger und wirft sie später doch
% wieder weg). Sie hat als wesentliche Aufgabe, den Kontext für die
% unterschiedlichen Klassen von Lesern herzustellen. Man muß hier die
% Leser für sich gewinnen. Das Problem, mit dem sich die Arbeit befaßt,
% sollte am Ende wenigsten in Grundzügen klar sein und dem Leser
% interessant erscheinen. Das Kapitel schließt mit einer Übersicht über
% den Rest der Arbeit. Meist braucht man mindestens 4 Seiten dafür, mehr
% als 10 Seiten liest keiner.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% INTRODUCTION                                                                 % 
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Checked
The domain of high performance computing is subject to constant
development.  The main focus of this development is the increase of
computing power \cite{ref}.  More computing power enables high performance
applications to solve more complex problems or to increase the problem
size.  Current state of the art computing systems reached the area of
one petaflop, that are $10^{15}$ floating point operations per second
(PFLOPS), in the year 2008 \cite{ref:ibm_roadrunner}. But, only a
small number of applications worldwide have taken advantage of these
so called petascale systems.

One application that has shown scalability on petascale systems, is
PIConGPU \cite{ref:picongpu_scale}. It is a particle in cell physics
simulation that was running on the Titan supercomputer at the Oak
Ridge National Laboratory in the United States \cite{ref:titan},
reaching 7.1 PFLOPS peak performance by utilizing 18.000 Graphics
Processing Units (GPUs). However, application capable of fully
utilizing a petascale system can often be expected to desire systems
with even more computational power.

Therefore, current computing systems will no longer adequately cover
the future needs of complex, computation intensive scientific and
industrial applications.  To provide more computing power to these
applications, a rapid progress in the development of computers and
algorithms is necessary. The next milestone in the development of
supercomputers are exascale systems, capable of at least $10^{18}$
floating point operations per second (exaflop/s), which represents a
thousandfold increase over the first petascale system.

Not all questions about the construction of such a system are
solved. Certainly, that exascale will increase the amount and
complexity of computers to a new level \cite{ref:cresta}, amplifying
the challenge of reliability, programmability and usability of such
systems.

Considering an exascale system, it is not sufficient to simply port
the applications previously running on petascale systems. The sole,
massive increase in size of the computing system will lead to a
decrease in the mean time to failure (MTTF) and an increased
importance of data locality.  Furthermore, the emergence of
accelerator hardware leads to computing systems that are more
heterogeneous and hierarchical, as well as increasingly complex to
program and use. Therefore, applications running on high performance
computers have to be adapted to utilize the upcoming generation of
supercomputers.  Current applications lack techniques that form the
basis for load balancing and fault tolerance.  Thus, it exists the
need for a smart description concept for high performance applications
that models upcoming exascale computing systems with respect to these
techniques.

Many applications exploit the parallel power of clusters by
interconnecting their parallel entities via network. These
applications, can be enhanced by a smart communication approach. This
approach will enhance already existing communication libraries by a
further layer, which is necessary for the upcoming
supercomputers. This additional layer is separated into three
components: an abstract communication layer, a model of the
communication topology, and a mapping from the topology onto the
abstract communication layer.  These mappings can be changed at any
time during the application execution. In connection with the
exchangeable communication library, it forms a flexible and portable
communication approach for the upcoming supercomputer generation. MPI
was used as back end underneath the communication abstraction layer to
implement a prototype that maps common communication processes
onto MPI methods.

The following Chapter~\ref{sec:state} is an introduction into the
current state of the art, giving a brief overview of the basic terms
in high performance computing and communication in computing
systems. Furthermore, it examines existing research in the area of
interest. Skilled readers can skip this section and continue straight
with the discussion about the design of the system components in
Chapter~\ref{sec:design}.  Selected implementation details of the
designed system is presented in Chapter~\ref{sec:implementation}. The
developed prototype is evaluated and compared in contrast to an MPI
implementation in Chapter~\ref{sec:evaluation}. Finally, ties
Chapter~\ref{sec:future_work} to the implementation by ideas of future
work.


\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
