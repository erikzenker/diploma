\chapter{Future Work}
\label{sec:futurework}

This chapter will first introduce ideas for enhancing the CAL by
further adapters. Subsequently, ideas for load balancing and fault
tolerance are presented. The discussed ideas should be a guide for
further development and call attention to the possibilities of the
GVON interface.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% MORE ADAPTERS                                                                %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation of Further Adapters}

The prototype implementation described in
Section~\ref{sec:implementation} utilized MPI as communication
back-end. While MPI is probably available on every computing system
and implements probably every important communication protocol, other
interesting communication libraries could be used as communication
back-end.  This section will discuss adapters based on different
communication libraries other than MPI.

% Internet Socket Based
\subsection*{Internet Socket Based Adapter}

Distributed applications, that are not deployed on clusters but on the
network of the internet, usually utilize communication libraries
different from MPI. Two interesting libraries are ZMQ~\cite{ref:zmq}
and Boost Asio~\cite{ref:boost_asio}, which are both based on the
TCP/IP and UDP/IP protocol stack. They can be used to interconnect
applications over the internet. Therefore, computations could be based
on the internet as communication network. Implementing an adapter for
the CAL that interfaces with ZMQ or Boost Asio would open an
application that is designed around the the GVON interface to the world of distributed
computing over the internet.

Projects for distributed computing such as Folding@home
\cite{ref:folding_at_home} and SETI@home \cite{ref:seti_at_home},
utilize the computing power of computers distributed all over the
world connected via the internet.  These applications distribute a
complex problem to a large number of peers. It would be interesting
to move an application from a cluster to a distributed environment
just by changing the CAL adapter (Figure~\ref{fig:internet_cal}).

\begin{figure}[H]
  \centering \includegraphics[width=\textwidth]{graphics/60_internet_cal}
  \caption{By exchanging the MPI adapter through an adapter based on
    internet sockets, an application initially designed for a cluster
    would also be executable as a distributed application over the
    internet.}
  \label{fig:internet_cal}
\end{figure}

\noindent Another use case is the connection of cluster nodes over the
internet. This would construct a grid computing environment based on
the GVON interface. Such an application could communicate via two CAL
objects, one local CAL configured by MPI and a global CAL configured
by internet sockets.  This approach could extend the computational
power of a cluster by further compute units located in other clusters.

% Adapters for Accelerators
\subsection*{Accelerator Based Adapter}

In contrast to the sender and receiver model of the communication over
a network such as the internet or a local area network, the
communication with and in between accelerator devices is mostly
modelled in a one-sided fashion. Therefore, the accelerator needs to
be managed from a host CPU.  This host needs to transform the
one-sided communication into a two-sided communication that is
supported by the developed system. Therefore, communication processes,
such as send and receive, between accelerators are managed by their
hosts.

A CAL adapter could model the offloading
mechanism of accelerators for CUDA~\cite{ref:cuda} and
OpenCL~\cite{ref:opencl} as communication processes.


 In the context of the
developed system, the pair of host and device forms a peer of the
CAL. Assume two computers A and B, each equipped with an accelerator, are
connected via a network. Transmitting data from one accelerator A to
other accelerator B (Figure~\ref{fig:acc_memcpy}) is split into the following steps:

\begin{enumerate}
\item A sends data to B (send)
  \begin{enumerate}
  \item Copy data from accelerator A's memory to host A's memory
  \item Send data to host B
  \end{enumerate}
\item B receives data from A (recv)
  \begin{enumerate}
  \item Receive data from host A
  \item Copy data from host B's memory to accelerator B's memory
  \end{enumerate}
\end{enumerate}

\begin{figure}[H]
  \begin{minipage}[t]{0.45\textwidth} 
    \includegraphics[width=\textwidth]{graphics/60_acc_memcpy}
    \caption{Exchanging data between accelerators. The data needs to be
    temporarily stored on the host's memories until the data is
    transmitted over a network connection.}
    \label{fig:acc_memcpy}
  \end{minipage}%
  \begin{minipage}[t]{0.1\textwidth}
    \hspace{0.1\textwidth}
  \end{minipage}%
  \begin{minipage}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{graphics/60_gpudirect}
    \caption{Direct exchange of data between accelerators over
      a network. The CPU memory is bypassed. NVIDIA provides such a
      technique called GPUDirect.}
    \label{fig:gpudirect}
  \end{minipage}%
  \\
  \\
  
% MPI on CUDA devices
\noindent The adapter design for accelerators mentioned above 
is required to take the detour above the host CPU. However, since CUDA version 4.0 and 5.0 there exist
techniques that allow to circumvent this detour. CUDA 4.0 introduced the Unified
Virtual Addressing (UVA).  It creates a uniform address space for the
host and all devices on a single node. Utilizing UVA removes the need for
explicit copies from and to accelerators.

The introduction of further GPUDirect features with CUDA 5.0 offers
first the possibility for direct exchange of data between accelerators
on the same node by peer-to-peer techniques.  Furthermore, data
can be exchanged between accelerators connected via a network by
remote direct memory access.  Both approaches bypass the CPU
and exchange data directly over the PCI bus or network controller
(Figure~\ref{fig:gpudirect}).



\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% MULTIPLE ADAPTER                                                             %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \section*{Multiple Adapter Design}
%% The possibility to exchange the adapter of the CAL also raises
%% the question if a design with more than one adapter at the same time
%% would be possible. Connecting the network of at least two adapters
%% would form a heterogeneous network, where each network provides 
%% properties like latency, bandwidth and hardware topology. The CAL
%% would unify these varying networks transparently under the same
%% interface. Thus, a multiple adapter design is the foundation for the
%% usage of the developed system in jungle computing environments (Section~\ref{sec:jungle}).

%% A CAL could be configured with an MPI adapter for communication inside
%% a cluster and with a internet socket adapter for communication over
%% the internet. This could connect two cluster systems over internet
%% to create a powerful tool to extend the computing power of a single
%% cluster.

%% Assuming a CAL configuration with two adapters mentioned above. Peers
%% that are present in the network of both adapters need to switch
%% between the adapters at run-time.  The CAL needs to provide a list of
%% all available adapters for a certain peer.  A particular sorting of
%% this list could state out the preferred adapter for communication
%% between two peers. Exchanging data between peers always requires a
%% look up of which adapter needs to be selected to address the
%% peers. The selection of a certain adapter can be performed at run-time
%% by a strategy pattern or partly at compile-time with variadic
%% templates.

%% Varying address spaces of the adapters have to be mapped onto the same
%% unified address space of the CAL.  This opens up the possibility that
%% two peers which do not share a same adapter, but are connected over a
%% third adapter, could still exchange data by routing over that third
%% adapter. Thus, a multiple adapter design does only make sense when
%% routing between two adapters is implemented.

%% Problems occur if collective operations are performed between varying
%% adapters. The collective operation has to be performed separately for
%% each adapter and then connected by a final collective operation.

%% Even if the design of a multiple adapter communication abstraction
%% layer looks quite interesting, the implementation is rather more
%% complex than a single adapter design. The decision for a single or
%% multiple adapter CAL should be placed in the hand of the application
%% developer.  Thus, the Cal could be configure by a further policy that
%% defines this adapter behavior.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% DEPLOYMENT IN REAL WORLD SIMULATION                                          %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deployment in Real World Simulations}

Section~\ref{sec:impl:gol} and \ref{sec:impl:nbody} have demonstrated that
implementations of Game of Life and N-body simulations on basis of the
GVON are possible. The performance evaluation in
Section~\ref{sec:eval:real} has shown that there exists no significant overhead with
respect to equivalent MPI implementations. Therefore, deploying the
developed system into real wold simulations will be the next step.

\subsection*{Deployment in PIConGPU}
In the beginning of this work, PIConGPU was the first simulation whose
communication processes were analyzed. However, the system was not
developed for the sole objective of the utilization in
PIConGPU. Therefore, it was developed independently from PIConGPU.
The next step for evaluating the developed system is the deployment of
the GVON as library into PIConGPU. This will evaluate how well the
developed system can be integrated into existing projects.

The PIConGPU communication processes are based on a three-dimensional
grid topology with diagonal connections. It resembles the
communication patterns used in the GoL simulation in Section~\ref{sec:gol}. Thus, a replacement
of communication related parts in the PIConGPU source code seems likely.

PIConGPU is interesting with respect to an accelerator-based adapter
mentioned above, because calculations in PIConGPU are offloaded to
GPUs. Therefore, bypassing the CPU in case of accelerator to
accelerator communication would increase the data transfer
performance.

% HASEonGPU
\subsection*{Deployment in HASEonGPU}
HASEonGPU is another project developed at the Helmholz Zentrum Dresden
Rossendorf (HZDR). HASEonGPU is a Monte-Carlo simulation of photons in
a laser gain medias. Whereby, the amplified spontaneous emission (ASE)
of photons is calculated for a set sample points in the simulated
volume of the gain media. The ASE value of each sample point can be
calculated independently from each other. Thus, a distribution onto a
cluster is possible.  The code for communication was never analyzed,
but it might be interesting if the developed communication approach
would fit on HASEonGPU.

The communication scheme works as follows: a master peer distributes the set of sample points onto
available worker peers. This forms a communication based on a
star topology, where the master peer is placed as star center. In a
scenario with high workload, e.g. lots of sample points, the
master peer could be the communication bottleneck.

However, utilizing the GVON would provide the possibility to exchange this
topology. The star topology could be replaced by a more general tree
topology. The master peer could delegate the distribution task to
sub-master peers that are responsible for sub-trees of worker peers.
Figure~\ref{fig:haseongpu_topology} shows the graph representation of
both the star topology and the tree topology. 

\begin{figure}[H]
  \centering \includegraphics[width=\textwidth]{graphics/60_haseongpu_topology}
  \caption{The exchange of the present star topology by a tree topology
  would increase the number of managing nodes that distribute workload.}
  \label{fig:haseongpu_topology}
\end{figure}

\noindent The number of sub-master peers could be adapted based on the
number of sample points.  Thus, more sample points would lead to an
increased width of the sub-master layer which leads to a better
workload distribution and reduces the bottleneck effect.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% GRAPH PARTITIONING                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Load Balancing Ideas}

% Load balancing
In the beginning of this work it was clarified, that load balancing
will not be the focus here. Therefore, only rudimentary
distribution algorithms such as round robin and consecutive distribution
were implemented. But it was also claimed that it should be possible
to build load balancing on top of the developed system.  The following
Sections will discuss ideas for load balancing based on the developed
system.

\subsection*{Graph Partitioning}

Assume, a simulation domain is modeled by a graph and the graph
contains more vertices than the number of peers in the global
context. These peers need to be oversubscribed by hosted vertices.
Therefore, the hosted vertices should be connected as close as
possible with respect to the graph. So that communication of hosted
vertices of the same host are as local as possible.  This potentially
minimizes the number of messages that are transferred over the
network.

% Graph partitioning
A well researched solution for this problem is partitioning of
graphs. Partitioning divides the a graph into smaller partitions.  A
good partitioning algorithm reduces the number of edges running
between partitions.  Since the communication topology of the GVON is
described by a graph, this graph can be partitioned by already
existing graph partitioning tools.

% Metis
\subsection*{Metis - a Graph Partitioning Tool}
The tool METIS and its related tools hMETIS and ParMETIS are
established applications for graph partitioning. The developers claim
that they provide high quality partitions, that their tools are two
magnitudes faster than other widely used partitioning algorithms, and
that graphs with several millions of vertices can be partitioned in
256 parts in a few seconds on current generation workstations and PCs.
The tool is published under the Apache Licence Version 2.0.,
therefore, METIS could support the graph distribution methods as
a library.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% DESCRIPTION OF ADAPTER HARDWARE TOPOLOGY                                     %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Description of the Adapter Hardware Topology}

% Cluster network topology
Cluster systems are equipped with varying network systems providing a
certain network topologies.  Utilizing the knowledge about the network
topology can increase application performance.  The most general
approach to describe the network topology of a cluster is a graph. A
network graph describes physical connections between nodes and could
be annotated with latency and bandwidth information.  This graph could
be the foundation of varying graph algorithm. For example could the
distance between two nodes be estimated and utilized to reduce
communication latency between peers.

With the existence of two graphs, one modeling the communication
topology of the simulation and another modeling the network
topology, a mapping between these graphs is possible.  Instead a
mapping from vertices to independent peers, vertices can be mapped
onto peers modeled in the network graph. Information that were used
to describe the network can be used to optimize this mapping.

In the oversubscribed case mentioned above, the communication graph could
be first partitioned to the number of available peers. After that, the
partitions will be mapped to the peers.  An optimal mapping would
result in an graph homomorphism. Deciding whether there exists a
homomorphism from one graph to another, is NP-complete. Therefore, a
heuristically approach that tries to maximize the amount of same
adjacent vertices in communication and network graph should be
chosen. Figure~\ref{fig:graph_partitioning} shows a partitioning of a
graph and a mapping of these partitions to peers.

\begin{figure}[H]
  \centering \includegraphics[width=\textwidth]{graphics/60_graph_partitioning}
  \caption{A two-dimensional grid is divided into three partitions.
    These partitions are mapped to the available peers with respect
    to their network topology. Each peer is oversubscribed by three
    vertices.}
  \label{fig:graph_partitioning}
\end{figure}

% Load balancing
\noindent A mapping between these two graphs opens new possibilities for
load balancing.  The most simple load balancing approach is a load
balancing at the initialization of an application. The communication
graph is statically mapped to the network graph. This
approach is sufficient for applications that do not change with
respect to load distribution and graph topology.

But load distribution of simulations can change during run-time. The
difference in workload among peers could be estimated by run-time
measurements of a single time step. The application needs to initiate
a rebalancing when the run-time of a host increases above a defined
threshold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% IDEAS FOR FAULT TOLERANCE                                                    %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fault Tolerance Ideas}

Similar to load balancing, fault tolerance of communication processes
was not a topic of this work. Nevertheless, the developed system
offers the possibility to implement fault tolerance techniques.

The main problem of a simulation is the failure of a peer during
run-time.  Some communication libraries such as MPI have the
unfortunate behavior that the whole program will fail, too. Therefore,
the simulation has to be restarted, if only a single peer fails.
Since, the amount of computing hardware increases with each new
cluster generation, it is not likely that a peer, host for
vertices of a simulation, fails. The library user has usually no
possibility to restart the failed peer, but the failed peer could be
replaced or even ignored in further communication processes.

% Reload from checkpoint
Assume, that each host checkpoints the state of its hosted
vertices at fixed points of the simulation execution. This
checkpoints could be written to hard disk of an network attached
storage or each host could distribute the data of its hosted
vertices to other host for backup reasons.

The network of hosts needs to notice the failure of hosts.
Furthermore, the hosts need to know where a failed host stored its
last saved state.  Some host has to adopt the hosted vertices of the
failed host.  This so-called adopter host has to load the saved states
of the adopted vertices before they will be announced together with
his already hosted vertices.  If the state of the loaded vertices is
not up-to-date, other hosts need to roll back the state of their
hosted vertices, too.  When all vertices are synchronized to the same
time step, the communication and ,hence , the simulation algorithm can
be continued.


% Redundant calculations
Redistribute the hosted vertices of a failed host to the remaining
hosts, increases their utilization.  Alternatively, the vertices could
be adopted by a backup peer.  Similar, to the previously mentioned
method, the backup peer needs to load the saved state from its adopted
vertices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% TWEAKS                                                                       %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Tweaks}
%
%\sitem{ Compile time generation of static graphs}
%\sitem{ Optimization of the graph to reduce overhead}
%\sitem{ Thread safe implementation of the gvon collectives}
%\sitem{ Multi-maps}


\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
