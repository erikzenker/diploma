\chapter{Future Work}
\label{sec:futurework}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% MORE ADAPTERS                                                                %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Further Adapters}

The prototype implementation (Section~\ref{sec:implementation})
utilized MPI as communication backend. While, MPI is probably
available on every computing system and implements probably every
important communication protocol, also other interesting communication
libraries could be used as communication back-end.  This Section will
discuss adapters based on a different communication library than MPI.

% Internet Socket Based
\subsection*{Internet Socket Based Adapter}

Especially distributed system,that are not deployed on clusters,
usually use other communication libraries than MPI. Two
interesting libraries are ZMQ and Boost Asio, which are both based
on the TCP/IP and UDP/IP protocol. They can be used to
interconnect applications over the internet. Therefore,
computations could be based on the internet as communication
network.Implementing an adapter for the CAL that interfaces with
ZMQ or Asio would open an application based on the GVON interface
to the world of distributed computing over the internet.

\todo{Illustrate}
Projects for distributed computing such as folding@home
\cite{ref:folding_at_home} and seti@home \cite{ref:seti_at_home},
utilize the computing power of computer distributed all over the
world connected via internet.  These application distribute a
complex problem to a big number of peers. It would be interesting
to move an application from a cluster to a distributed environment
by just changing the CAL adapter.

A further use case is the connection of cluster nodes over the
internet. This would construct a grid computing environment based
on the GVON interface. The application could communicate via two
CAL instantiations, one local CAL configured by MPI and a global
CAL configured by internet sockets.  It could easily extend the
computational power of a cluster by further computing
units.

% Adapters for Accelerators
\subsection*{Accelerator Based Adapter}

In contrast to communication over a network such as the internet
or a local area network, is the modeling of the communication with
and in between accelerator devices more similar to memory copies.
An adapter could model the offload mechanism of accelerators for
CUDA and OpenCL as communication processes.

Since, offload is a one-sided communication concept, the
accelerator device needs to be managed from a host CPU. This host
needs to transform the one-sided communication into a two-sided
communication that is supported by the developed system.

Therefore, Communication processes, such as send and receive,
between accelerators are managed by their hosts. In the world of the
developed system, the pair of host and device form a peer of the
CAL. Assume, two computers each equipped with an accelerator are
connected via a network. Transmitting data from one accelerator A to
another accelerator B is separated into the following steps:

\todo{This might be better as graphic}
\begin{enumerate}
\item A Send data to B
  \begin{itemize}
  \item Copy of data from A memory to host memory
  \item Transmission to host of B
  \end{itemize}
\item B Receive data from A
  \begin{itemize}
  \item Receive of data from host of A
  \item Copy of data from host memory to B memory
  \end{itemize}
\end{enumerate}

% MPI on CUDA devices
The adapter design for accelerators mentioned above has the drawback
that data has to take the detour above the host CPU. But, the emerge
of CUDA version 4.0 and 5.0 provides techniques that remove the need
for the detour.

First of all, introduces CUDA 4.0 the Unified Virtual Addressing
(UVA).  It creates a uniform address space of and devices of a
single node. Utilizing UVA removes the need for explicit copies
from and to accelerators.

The introduction of GPUDirect offers a direct exchange of data
between accelerators on the same node (P2P) and of data between
accelerator connected by a network (RDMA).  Both approaches bypass
the CPU and exchange data directly over the PCI bus or network
controller.

\todo{Graphic of CUDA with GPUDirect}

\todo{``Provide small example''}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% MULTIPLE ADAPTER                                                             %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Multiple Adapter Design}
\sitem{Until now, the adapter design is chosen deliberately}

But, the possibility to exchange the adapter of the CAL also raises
the question if a design with more than one adapter at the same time
would be possible. Connecting the network of at least two adapters
would form a heterogeneous network, where each network has its own
properties like latency, bandwidth and hardware topology. The CAL
would unify these varying networks transparently under the same
interface. Thus, a multi adapter design is the foundation for the
usage of the developed system in jungle computing environments
\ref{sec:jungle}.


\sitem{A CAL with multiple adapters could connect two cluster systems
  over internet.}

\sitem{This would create a powerful tool to extend the computing power
  of a single cluster}

\sitem{Easily create grid computing structures}

Assume, the CAL has two adapters, one for MPI and another for sockets,
to chose from and some peers of the network are able to communicate
through both adapters.  The CAL needs to provide a list of all
avaiblabe adapters for a specific peer. A particular sorting of this
list could state out the prefered adapter for communication between
two peers. Exchanging data between peers always requires a lookup of
which adapter needs to be selected to address the peers.

\sitem{On the implementation side, there are two solution to switch
  between adapters at run-time}

\sitem{strategy patter, is the run-time solution}

\todo{Inform about strategy pattern}

\sitem{variadic templates, is the compile-time solution}

Furthermore, varying real addresses spaces of different adapters have
to be mapped onto the same unified address space of the CAL.  It opens
up the possibility that two peers which do not share a same adapter,
but are connected over a third adapter, could still exchange data by
routing over that third adapter. Thus, a multi adapter design does
only make sense when routing between two adapters is implemented.

\sitem{Same problem with collective operations between varying
  adapters}

\sitem{Support of collective operations over the borders of different
  adapters in the case that the CAL supports more than one adapter
  being instantiated.}

Even if the design of a multi adapter communication abstraction layer
looks quite interesting, the implementation is rather more complex
than a single adapter design. The decision for a single or multiple
adapter CAL should be placed in the hand of the application developer.
Thus, the developer could configure the CAL by a further policy that
defines the adapter behavior.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% DEPLOYMENT IN REAL WORLD SIMULATION                                          %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Deployment in Real World Simulations}

Section~\ref{sec:impl:gol} and \ref{sec:impl:nbody} have shown
that implementations of Game of Life and N-body simulations on basis
of the GVON are possible. And the performance evaluation in
Section~\ref{sec:eval:real} have shown no significant overhead in
respect to equivalent MPI implementations. Therefore, deploying
the developed system into real wold simulations would be the next
step.

\subsection*{Deployment in PIConGPU}
In the beginning of this work was PIConGPU the first simulation
whose communication processes were analyzed. But the system was not
developed for the sole objective for utilization in
PIConGPU. Therefore, it was developed independently.  The next step
for evaluating the developed system is the deployment of the GVON as
library into PIConGPU. This will show how well the developed system
can be integrated into existing projects.

The PIConGPU communication processes are is based on a
three-dimensional grid topology with diagonal connections. It does
not differ to much from communication patterns used in the GoL
simulation. Thus, a replacement of communication related part in
PIConGPU source code should be easily possible.

PIConGPU is also interesting with respect of an CUDA-aware
adapter.  Most calculation in PIConGPU are offloaded to the
CPUs. Therefore, bypassing the CPU in case of device to device
copies would increase overall performance.

% HASEonGPU
\subsection*{Deployment in HASEonGPU}
HASEonGPU is another project developed at the Helmholz Zentrum
Dresden Rossendorf.  It was never an object of communication code
analyses, but it would be interesting if the developed communication
approach would easily fit on HASEonGPU.

HASEonGPU is Monte-Carlo simulation of photons in a laser gain
medias. Whereby, the amplified spontaneous emission (ASE) of photons
is calculated for certain sample points in the simulated volume of
the gain media.The ASE value of each sample point can be calculated
independently from each other. Thus, a distribution onto a cluster
is possible.

A master peer distributes sample points onto
available worker peers. This forms a communication based on a
star-topology, where the master peer is placed as star center. In a
scenario with a lot of workload, e.g. by lot of sample points, the
master peer would be the communication bottleneck.

But, the utilization of the GVON would make an exchange of this
topology very easy. The star-topology could be replaced by a more
general tree topology. The master peer could delegate distribution
task to sub-master peers that are responsible for sub-trees.

\todo{Show graphic, how to exchange star topology by tree topology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% GRAPH PARTITIONING                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Graph Partitioning}

% Load balancing
In the beginning of this work it was clarified, that load balancing
will be not the focus of this work. Therefore, only rudimentary
distribution algorithms such round robin and consecutive
distribution were implemented. But, it was also claimed, that it
should be possible to build load balancing on top. In conjunction
with the following Section will this Section discuss ideas for load
balancing based on the developed system.

Assume, a simulation domain is modeled by a graph and the graph
contains more vertices than peers available in the global
context. These peers need to be oversubscribed by hosted vertices.
Therefore, the hosted vertices should be connected as
close as possible with respect to the graph. So that, communication
between vertices of the same host are as local as possible. This
potentially minimizes the amount of messages that have to be
transferred over the network.

% Graph partitioning
A well researched solution for this problem is partitioning of
graphs. Partitioning divides the a graph into smaller components.  A
good partition is defined as one in which the number of edges
running between separated components is small. Since the
communication topology is described by a graph, it can be easily
partitioned by already existing graph partitioning tools.

% Metis
\subsection*{Metis as Graph Partitioning Tool}
The tool METIS and its related tools hMETIS and ParMETIS are
established applications for graph partitioning. They provide high
quality partitions and are two magnitudes faster than other widely
used partitioning algorithms compared to other graph partitioning
tools. The developers claim that graphs with several millions of
vertices can be partitioned in 256 parts in a few seconds on current
generation workstations and PCs.  Furthermore, because the tool is
published under the Apache Licence Version 2.0. , metis could
support as library the graph distribution methods.

\todo{Which format does Metis need}
\todo{How can it be deployed in existing software}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% DESCRIPTION OF ADAPTER HARDWARE TOPOLOGY                                     %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Description of the Adapter Hardware Topology}

% Cluster network topology
Cluster systems are equipped with varying network systems. Usually
with custom build network topologies.  Utilizing the knowledge about
the network topology can increase application performance.  The most
general approach to describe the network topology of a cluster is a
graph. A network graph describes physical connections between nodes
and could be annotated with latency and bandwidth information.  This
graph could be the foundation of varying graph algorithm. For
example could the distance between two nodes be estimated and
utilized to reduce communication latency between peers.

With the existence of two graphs, one modeling the communication
topology of the simulation and another modeling the network
topology, a mapping between these graphs is possible.  Instead a
mapping from vertices to independent peers, vertices can be mapped
onto peers modeled in the network graph. Information that were used
to describe the network can be used to optimize this mapping.

In an oversubscribed case, could the communication graph first be
partitioned to the number of available peers. After that, the
partitions will be mapped onto the peers.  An optimal mapping would
result in an graph homomorphism. Deciding whether there exists a
homomorphism from one graph to another, is NP-complete. Therefore, a
heuristically approach that tries to maximize the amount of same
adjacent vertices in communication and network graph should be
chosen.

% Load balancing
A Mapping between the two graphs mentioned above opens new
possibilities for load balancing.  The most simple load balancing
approach is a load balancing at the initialization of an
application. The communication graph is mapped only once and
statically onto the network graph. This approach is sufficient for
applications that do not change with respect to load distribution
and graph topology.

But load distribution of simulations can change during run-time. The
difference in workload among peers could be estimated by run-time
measurements of a single time-step. The application needs to
initiate a rebalancing when the run-time of a host increases above a
defined threshold.  But, a static load balancing could also be
performed periodically every n time-steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% IDEAS FOR FAULT TOLERANCE                                                    %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Ideas for Fault Tolerance}

 Similar to load balancing was fault tolerance of communication
  processes no topic for this work. Nevertheless, offers the developed
  system the possibility to build fault tolerance techniques on top.

  The main problem is the failure of a peer during execution of
  simulation.  Some communication libraries such as MPI have the
  embarrassing behavior that the whole program will fail
  too. Therefore, the simulation has to be restarted, if only a single
  peer fails.  Since, the amount of computing hardware increases with
  each new cluster generation, it is not unlikely that a peer, that is
  host for vertices of a simulation, fails. The library user has
  usually no possibility to restart the failed peer, but the failed
  peer could be ignored for further communication processes.

  %Reload from checkpoint
 Assume, that the host checkpoints the state of its hosted
  vertices at fixed points of the simulation execution. This
  checkpoints could be written onto hard disk of an network attached
  storage (NAS) or each peer could distribute the data of its hosted
  vertices to other peers for backup reasons.

 The network of peers needs to notice the failure of the peer.
  Furthermore, the peers need estimate where the failed peer stored its
  checkpointed states.

 Some peer has to adopt the hosted vertices of the failed host.
  This adopter peer has to announce the adopted vertices together
  with his already hosted vertices.

 The adopter peer has to load the checkpointed states of the
  adopted vertices. In dependence whether the checkpointed state is
  up-to-date, other host do also need to load checkpointed
  states. When all vertices are synchronized to the same time-step,
  the communication and therefore the simulation algorithm can be
  continued.

\todo{Make graphics that illustrates the fault tolerance idea}

% Redundant calculations
 Redistribute the hosted vertices of a failed host onto the
  remaining host increases the workload each host has to manage.
  Instead, the vertices could be adopted by a backup peer that has no
  hosted vertices so far.  Equally, to the method mentioned above,
  needs the backup peer to load the checkpointed state from its
  adopted vertices

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% TWEAKS                                                                       %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Tweaks}

\sitem{ Compile time generation of static graphs}
\sitem{ Optimization of the graph to reduce overhead}
\sitem{ Thread safe implementation of the gvon collectives}
\sitem{ Multi-maps}


\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
